{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4d1c036-b4d0-4559-8061-c96ccac15a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from scipy.io import loadmat\n",
    "from scipy.special import ellipj, ellipk\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24d933bf-a837-41f5-b073-2e5fde1bb284",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = 0.03\n",
    "theta = 2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e852730-87d9-46ce-a7a6-8ef5adc0c5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pendulum(noise, theta=2.4):\n",
    "    \n",
    "    np.random.seed(1)\n",
    "\n",
    "    def sol(t,theta0):\n",
    "        S = np.sin(0.5*(theta0) )\n",
    "        K_S = ellipk(S**2)\n",
    "        omega_0 = np.sqrt(9.81)\n",
    "        sn,cn,dn,ph = ellipj( K_S - omega_0*t, S**2 )\n",
    "        theta = 2.0*np.arcsin( S*sn )\n",
    "        d_sn_du = cn*dn\n",
    "        d_sn_dt = -omega_0 * d_sn_du\n",
    "        d_theta_dt = 2.0*S*d_sn_dt / np.sqrt(1.0-(S*sn)**2)\n",
    "        return np.stack([theta, d_theta_dt],axis=1)\n",
    "    \n",
    "    \n",
    "    anal_ts = np.arange(0, 2200*0.1, 0.1)\n",
    "    X = sol(anal_ts, theta)\n",
    "    \n",
    "    X = X.T\n",
    "    Xclean = X.copy()\n",
    "    X += np.random.standard_normal(X.shape) * noise\n",
    "    \n",
    "    \n",
    "    # Rotate to high-dimensional space\n",
    "    Q = np.random.standard_normal((64,2))\n",
    "    Q,_ = np.linalg.qr(Q)\n",
    "    \n",
    "    X = X.T.dot(Q.T) # rotate\n",
    "    Xclean = Xclean.T.dot(Q.T)\n",
    "    \n",
    "    # scale \n",
    "    X = 2 * (X - np.min(X)) / np.ptp(X) - 1\n",
    "    Xclean = 2 * (Xclean - np.min(Xclean)) / np.ptp(Xclean) - 1\n",
    "\n",
    "    \n",
    "    # split into train and test set \n",
    "    X_train = X[0:600]   \n",
    "    X_test = X[600:]\n",
    "\n",
    "    X_train_clean = Xclean[0:600]   \n",
    "    X_test_clean = Xclean[600:]     \n",
    "    \n",
    "    #******************************************************************************\n",
    "    # Return train and test set\n",
    "    #******************************************************************************\n",
    "    return X_train, X_test, X_train_clean, X_test_clean, 64, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ace44b5a-e3d1-491f-894d-554e0d1eb164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, steps):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.steps = steps\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, mode = \"forward\"):\n",
    "        # h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        # c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "      \n",
    "    \n",
    "        q= x\n",
    "        output_list= []\n",
    "        output_back_list = []\n",
    "        if mode == 'forward':\n",
    "            h = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "            c = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "            for _ in range(self.steps):\n",
    "                # print(\"q\",q.shape)\n",
    "                out, _ = self.lstm(q,(h,c))\n",
    "                out = self.fc(out)\n",
    "                q =out\n",
    "                output_list.append(q)\n",
    "            output_list.append(x) \n",
    "        \n",
    "        return output_list,output_back_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13eec590-544a-4ec8-bb51-522e1508d5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([600, 1, 64])\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Xtrain_clean, Xtest_clean, m, n = pendulum(noise=noise, theta=theta)\n",
    "\n",
    "\n",
    "X_train = X_train.reshape( X_train.shape[0], 1, X_train.shape[1])\n",
    "X_train = torch.from_numpy(X_train).float().contiguous()\n",
    "\n",
    "print (X_train.shape)\n",
    "trainDat = []\n",
    "start = 0\n",
    "steps = 6\n",
    "\n",
    "for i in np.arange(steps,-1, -1):\n",
    "    if i == 0:\n",
    "        trainDat.append(X_train[start:])\n",
    "    else:\n",
    "        trainDat.append(X_train[start:-i])\n",
    "    start += 1\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(*trainDat)\n",
    "# train_data = torch.tensor(trainDat)\n",
    "del(trainDat)\n",
    "\n",
    "train_loader = DataLoader(dataset = train_data,batch_size = 64,shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5d9119f-3f3b-4c10-982d-9f5de3c2ceac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/500], Step [10/10], Loss: 3.8542795181274414\n",
      "Epoch [4/500], Step [10/10], Loss: 3.5896878242492676\n",
      "Epoch [6/500], Step [10/10], Loss: 2.2423147708177567\n",
      "Epoch [8/500], Step [10/10], Loss: 1.4777722209692001\n",
      "Epoch [10/500], Step [10/10], Loss: 1.3206848502159119\n",
      "Epoch [12/500], Step [10/10], Loss: 1.2234707549214363\n",
      "Epoch [14/500], Step [10/10], Loss: 1.161753587424755\n",
      "Epoch [16/500], Step [10/10], Loss: 1.0547170042991638\n",
      "Epoch [18/500], Step [10/10], Loss: 0.8735829889774323\n",
      "Epoch [20/500], Step [10/10], Loss: 0.27444878220558167\n",
      "Epoch [22/500], Step [10/10], Loss: 0.08601407334208488\n",
      "Epoch [24/500], Step [10/10], Loss: 0.031617759028449655\n",
      "Epoch [26/500], Step [10/10], Loss: 0.014394201920367777\n",
      "Epoch [28/500], Step [10/10], Loss: 0.008381827967241406\n",
      "Epoch [30/500], Step [10/10], Loss: 0.005927443620748818\n",
      "Epoch [32/500], Step [10/10], Loss: 0.005023412843002006\n",
      "Epoch [34/500], Step [10/10], Loss: 0.004448940570000559\n",
      "Epoch [36/500], Step [10/10], Loss: 0.004040821484522894\n",
      "Epoch [38/500], Step [10/10], Loss: 0.003882524586515501\n",
      "Epoch [40/500], Step [10/10], Loss: 0.003636226028902456\n",
      "Epoch [42/500], Step [10/10], Loss: 0.0035287834762129933\n",
      "Epoch [44/500], Step [10/10], Loss: 0.003314157191198319\n",
      "Epoch [46/500], Step [10/10], Loss: 0.003247987071517855\n",
      "Epoch [48/500], Step [10/10], Loss: 0.0031005599012132734\n",
      "Epoch [50/500], Step [10/10], Loss: 0.003019922674866393\n",
      "Epoch [52/500], Step [10/10], Loss: 0.0029524778365157545\n",
      "Epoch [54/500], Step [10/10], Loss: 0.0030197301239240915\n",
      "Epoch [56/500], Step [10/10], Loss: 0.002919828752055764\n",
      "Epoch [58/500], Step [10/10], Loss: 0.002772883453872055\n",
      "Epoch [60/500], Step [10/10], Loss: 0.002949481611722149\n",
      "Epoch [62/500], Step [10/10], Loss: 0.002918457190389745\n",
      "Epoch [64/500], Step [10/10], Loss: 0.0027221833151998\n",
      "Epoch [66/500], Step [10/10], Loss: 0.002469657643814571\n",
      "Epoch [68/500], Step [10/10], Loss: 0.002360827478696592\n",
      "Epoch [70/500], Step [10/10], Loss: 0.0023666734050493687\n",
      "Epoch [72/500], Step [10/10], Loss: 0.0022651025938102975\n",
      "Epoch [74/500], Step [10/10], Loss: 0.002265576767968014\n",
      "Epoch [76/500], Step [10/10], Loss: 0.002145536316675134\n",
      "Epoch [78/500], Step [10/10], Loss: 0.0020721726905321702\n",
      "Epoch [80/500], Step [10/10], Loss: 0.0021690909925382584\n",
      "Epoch [82/500], Step [10/10], Loss: 0.0019829564407700673\n",
      "Epoch [84/500], Step [10/10], Loss: 0.00200081396906171\n",
      "Epoch [86/500], Step [10/10], Loss: 0.0020081163092982024\n",
      "Epoch [88/500], Step [10/10], Loss: 0.002116672316333279\n",
      "Epoch [90/500], Step [10/10], Loss: 0.0019977677875431255\n",
      "Epoch [92/500], Step [10/10], Loss: 0.002023870139964856\n",
      "Epoch [94/500], Step [10/10], Loss: 0.0020224919862812385\n",
      "Epoch [96/500], Step [10/10], Loss: 0.0017727394442772493\n",
      "Epoch [98/500], Step [10/10], Loss: 0.002679992132470943\n",
      "Epoch [100/500], Step [10/10], Loss: 0.0017818345804698765\n",
      "Epoch [102/500], Step [10/10], Loss: 0.0017320254410151392\n",
      "Epoch [104/500], Step [10/10], Loss: 0.0018535208655521274\n",
      "Epoch [106/500], Step [10/10], Loss: 0.0018854902737075463\n",
      "Epoch [108/500], Step [10/10], Loss: 0.0016852534899953753\n",
      "Epoch [110/500], Step [10/10], Loss: 0.001804915998945944\n",
      "Epoch [112/500], Step [10/10], Loss: 0.002053355870884843\n",
      "Epoch [114/500], Step [10/10], Loss: 0.002427867890219204\n",
      "Epoch [116/500], Step [10/10], Loss: 0.0016611509781796485\n",
      "Epoch [118/500], Step [10/10], Loss: 0.0016151938762050122\n",
      "Epoch [120/500], Step [10/10], Loss: 0.0016217337979469448\n",
      "Epoch [122/500], Step [10/10], Loss: 0.0016684896691003814\n",
      "Epoch [124/500], Step [10/10], Loss: 0.0017392049194313586\n",
      "Epoch [126/500], Step [10/10], Loss: 0.0021504310134332627\n",
      "Epoch [128/500], Step [10/10], Loss: 0.0017198560817632824\n",
      "Epoch [130/500], Step [10/10], Loss: 0.0017224177427124232\n",
      "Epoch [132/500], Step [10/10], Loss: 0.002085621759761125\n",
      "Epoch [134/500], Step [10/10], Loss: 0.002079230602248572\n",
      "Epoch [136/500], Step [10/10], Loss: 0.001685917959548533\n",
      "Epoch [138/500], Step [10/10], Loss: 0.001724984307656996\n",
      "Epoch [140/500], Step [10/10], Loss: 0.0014975719386711717\n",
      "Epoch [142/500], Step [10/10], Loss: 0.001972530604689382\n",
      "Epoch [144/500], Step [10/10], Loss: 0.0025358151324326172\n",
      "Epoch [146/500], Step [10/10], Loss: 0.0015039247809909284\n",
      "Epoch [148/500], Step [10/10], Loss: 0.0020224376785336062\n",
      "Epoch [150/500], Step [10/10], Loss: 0.0016955502360360697\n",
      "Epoch [152/500], Step [10/10], Loss: 0.0014098338797339238\n",
      "Epoch [154/500], Step [10/10], Loss: 0.0012888782221125439\n",
      "Epoch [156/500], Step [10/10], Loss: 0.0015289537332137115\n",
      "Epoch [158/500], Step [10/10], Loss: 0.0013276980535010807\n",
      "Epoch [160/500], Step [10/10], Loss: 0.0013053067159489729\n",
      "Epoch [162/500], Step [10/10], Loss: 0.0016237554300460033\n",
      "Epoch [164/500], Step [10/10], Loss: 0.0014313325082184747\n",
      "Epoch [166/500], Step [10/10], Loss: 0.001557277653773781\n",
      "Epoch [168/500], Step [10/10], Loss: 0.0016183722400455736\n",
      "Epoch [170/500], Step [10/10], Loss: 0.0014129298069747165\n",
      "Epoch [172/500], Step [10/10], Loss: 0.0013114029643475078\n",
      "Epoch [174/500], Step [10/10], Loss: 0.0013927512773079798\n",
      "Epoch [176/500], Step [10/10], Loss: 0.0012353161728242412\n",
      "Epoch [178/500], Step [10/10], Loss: 0.001134918500611093\n",
      "Epoch [180/500], Step [10/10], Loss: 0.001356583132292144\n",
      "Epoch [182/500], Step [10/10], Loss: 0.0014186529006110504\n",
      "Epoch [184/500], Step [10/10], Loss: 0.0011302693237666972\n",
      "Epoch [186/500], Step [10/10], Loss: 0.0014389975622179918\n",
      "Epoch [188/500], Step [10/10], Loss: 0.0012828360922867432\n",
      "Epoch [190/500], Step [10/10], Loss: 0.0016823695186758414\n",
      "Epoch [192/500], Step [10/10], Loss: 0.0015226028335746378\n",
      "Epoch [194/500], Step [10/10], Loss: 0.001273099987884052\n",
      "Epoch [196/500], Step [10/10], Loss: 0.001903622323879972\n",
      "Epoch [198/500], Step [10/10], Loss: 0.001232416063430719\n",
      "Epoch [200/500], Step [10/10], Loss: 0.0015637747928849421\n",
      "Epoch [202/500], Step [10/10], Loss: 0.001285681915760506\n",
      "Epoch [204/500], Step [10/10], Loss: 0.001153600969701074\n",
      "Epoch [206/500], Step [10/10], Loss: 0.0013159026348148473\n",
      "Epoch [208/500], Step [10/10], Loss: 0.0018220102647319436\n",
      "Epoch [210/500], Step [10/10], Loss: 0.0012954620324308053\n",
      "Epoch [212/500], Step [10/10], Loss: 0.001489316469815094\n",
      "Epoch [214/500], Step [10/10], Loss: 0.0036575237318174914\n",
      "Epoch [216/500], Step [10/10], Loss: 0.00258224896970205\n",
      "Epoch [218/500], Step [10/10], Loss: 0.002043521875748411\n",
      "Epoch [220/500], Step [10/10], Loss: 0.0014662601461168379\n",
      "Epoch [222/500], Step [10/10], Loss: 0.0013483832517522387\n",
      "Epoch [224/500], Step [10/10], Loss: 0.0013481916030286811\n",
      "Epoch [226/500], Step [10/10], Loss: 0.0012520556338131428\n",
      "Epoch [228/500], Step [10/10], Loss: 0.001175787125248462\n",
      "Epoch [230/500], Step [10/10], Loss: 0.0011760648339986801\n",
      "Epoch [232/500], Step [10/10], Loss: 0.002128549327608198\n",
      "Epoch [234/500], Step [10/10], Loss: 0.0014908298617228866\n",
      "Epoch [236/500], Step [10/10], Loss: 0.0018821727062459104\n",
      "Epoch [238/500], Step [10/10], Loss: 0.0017331373310298659\n",
      "Epoch [240/500], Step [10/10], Loss: 0.0015545225323876366\n",
      "Epoch [242/500], Step [10/10], Loss: 0.001679538079770282\n",
      "Epoch [244/500], Step [10/10], Loss: 0.0011402601521695033\n",
      "Epoch [246/500], Step [10/10], Loss: 0.001244648257852532\n",
      "Epoch [248/500], Step [10/10], Loss: 0.0012482640086091124\n",
      "Epoch [250/500], Step [10/10], Loss: 0.002192701620515436\n",
      "Epoch [252/500], Step [10/10], Loss: 0.0014478806988336146\n",
      "Epoch [254/500], Step [10/10], Loss: 0.0011685061690513976\n",
      "Epoch [256/500], Step [10/10], Loss: 0.001214500553032849\n",
      "Epoch [258/500], Step [10/10], Loss: 0.0011438373621786013\n",
      "Epoch [260/500], Step [10/10], Loss: 0.001229911737027578\n",
      "Epoch [262/500], Step [10/10], Loss: 0.0012757155200233683\n",
      "Epoch [264/500], Step [10/10], Loss: 0.0018392365309409797\n",
      "Epoch [266/500], Step [10/10], Loss: 0.0015123642224352807\n",
      "Epoch [268/500], Step [10/10], Loss: 0.0011295367366983555\n",
      "Epoch [270/500], Step [10/10], Loss: 0.0014311046470538713\n",
      "Epoch [272/500], Step [10/10], Loss: 0.0011072487541241571\n",
      "Epoch [274/500], Step [10/10], Loss: 0.0015008086629677564\n",
      "Epoch [276/500], Step [10/10], Loss: 0.0021867155155632645\n",
      "Epoch [278/500], Step [10/10], Loss: 0.0014805687897023745\n",
      "Epoch [280/500], Step [10/10], Loss: 0.0012882595110568218\n",
      "Epoch [282/500], Step [10/10], Loss: 0.0015520987726631574\n",
      "Epoch [284/500], Step [10/10], Loss: 0.001172721094917506\n",
      "Epoch [286/500], Step [10/10], Loss: 0.0010831864638021216\n",
      "Epoch [288/500], Step [10/10], Loss: 0.0011482930567581207\n",
      "Epoch [290/500], Step [10/10], Loss: 0.001219065197801683\n",
      "Epoch [292/500], Step [10/10], Loss: 0.00189115091052372\n",
      "Epoch [294/500], Step [10/10], Loss: 0.005717928375815973\n",
      "Epoch [296/500], Step [10/10], Loss: 0.0037470538372872397\n",
      "Epoch [298/500], Step [10/10], Loss: 0.001913963074912317\n",
      "Epoch [300/500], Step [10/10], Loss: 0.0013640746619785205\n",
      "Epoch [302/500], Step [10/10], Loss: 0.0013040696503594518\n",
      "Epoch [304/500], Step [10/10], Loss: 0.0013418362141237594\n",
      "Epoch [306/500], Step [10/10], Loss: 0.001411248667864129\n",
      "Epoch [308/500], Step [10/10], Loss: 0.0013667431703652255\n",
      "Epoch [310/500], Step [10/10], Loss: 0.0012295245178393088\n",
      "Epoch [312/500], Step [10/10], Loss: 0.0016225450017373078\n",
      "Epoch [314/500], Step [10/10], Loss: 0.0016705894158803858\n",
      "Epoch [316/500], Step [10/10], Loss: 0.00112019829248311\n",
      "Epoch [318/500], Step [10/10], Loss: 0.0010115776167367585\n",
      "Epoch [320/500], Step [10/10], Loss: 0.0010767719068098813\n",
      "Epoch [322/500], Step [10/10], Loss: 0.0009737606451380998\n",
      "Epoch [324/500], Step [10/10], Loss: 0.001026644094963558\n",
      "Epoch [326/500], Step [10/10], Loss: 0.001066719793016091\n",
      "Epoch [328/500], Step [10/10], Loss: 0.0012023807139485143\n",
      "Epoch [330/500], Step [10/10], Loss: 0.0013120560033712536\n",
      "Epoch [332/500], Step [10/10], Loss: 0.0011857424178742804\n",
      "Epoch [334/500], Step [10/10], Loss: 0.001192681782413274\n",
      "Epoch [336/500], Step [10/10], Loss: 0.0012737003635265864\n",
      "Epoch [338/500], Step [10/10], Loss: 0.0011155701067764312\n",
      "Epoch [340/500], Step [10/10], Loss: 0.0015257405320880935\n",
      "Epoch [342/500], Step [10/10], Loss: 0.0011304480431135744\n",
      "Epoch [344/500], Step [10/10], Loss: 0.0012293227628106251\n",
      "Epoch [346/500], Step [10/10], Loss: 0.0010025882002082653\n",
      "Epoch [348/500], Step [10/10], Loss: 0.0011818762141047046\n",
      "Epoch [350/500], Step [10/10], Loss: 0.0012905982803204097\n",
      "Epoch [352/500], Step [10/10], Loss: 0.0012429039561538957\n",
      "Epoch [354/500], Step [10/10], Loss: 0.0011728513563866727\n",
      "Epoch [356/500], Step [10/10], Loss: 0.0016201396065298468\n",
      "Epoch [358/500], Step [10/10], Loss: 0.0012002694711554796\n",
      "Epoch [360/500], Step [10/10], Loss: 0.0011296859447611496\n",
      "Epoch [362/500], Step [10/10], Loss: 0.0010286456381436437\n",
      "Epoch [364/500], Step [10/10], Loss: 0.0011244047054788098\n",
      "Epoch [366/500], Step [10/10], Loss: 0.0016591909734415822\n",
      "Epoch [368/500], Step [10/10], Loss: 0.0009469572905800305\n",
      "Epoch [370/500], Step [10/10], Loss: 0.0010220160766039044\n",
      "Epoch [372/500], Step [10/10], Loss: 0.0013440947368508205\n",
      "Epoch [374/500], Step [10/10], Loss: 0.0013303795640240423\n",
      "Epoch [376/500], Step [10/10], Loss: 0.0012563480995595455\n",
      "Epoch [378/500], Step [10/10], Loss: 0.001197978577692993\n",
      "Epoch [380/500], Step [10/10], Loss: 0.0009966798825189471\n",
      "Epoch [382/500], Step [10/10], Loss: 0.0017067118169507012\n",
      "Epoch [384/500], Step [10/10], Loss: 0.001717511608148925\n",
      "Epoch [386/500], Step [10/10], Loss: 0.0014876874556648545\n",
      "Epoch [388/500], Step [10/10], Loss: 0.0015276725534931757\n",
      "Epoch [390/500], Step [10/10], Loss: 0.001681864625425078\n",
      "Epoch [392/500], Step [10/10], Loss: 0.001845579463406466\n",
      "Epoch [394/500], Step [10/10], Loss: 0.0016048089892137796\n",
      "Epoch [396/500], Step [10/10], Loss: 0.0018686059120227583\n",
      "Epoch [398/500], Step [10/10], Loss: 0.0013125803307048045\n",
      "Epoch [400/500], Step [10/10], Loss: 0.0011530821066116914\n",
      "Epoch [402/500], Step [10/10], Loss: 0.00123509498371277\n",
      "Epoch [404/500], Step [10/10], Loss: 0.00196119406609796\n",
      "Epoch [406/500], Step [10/10], Loss: 0.0012235838948981836\n",
      "Epoch [408/500], Step [10/10], Loss: 0.001082080416381359\n",
      "Epoch [410/500], Step [10/10], Loss: 0.001003123565169517\n",
      "Epoch [412/500], Step [10/10], Loss: 0.0012407732356223278\n",
      "Epoch [414/500], Step [10/10], Loss: 0.0017135700327344239\n",
      "Epoch [416/500], Step [10/10], Loss: 0.0016461725826957263\n",
      "Epoch [418/500], Step [10/10], Loss: 0.0013790720331599005\n",
      "Epoch [420/500], Step [10/10], Loss: 0.001251400783075951\n",
      "Epoch [422/500], Step [10/10], Loss: 0.001256272902537603\n",
      "Epoch [424/500], Step [10/10], Loss: 0.0012891322548966855\n",
      "Epoch [426/500], Step [10/10], Loss: 0.0011462642141850665\n",
      "Epoch [428/500], Step [10/10], Loss: 0.0012081917739124037\n",
      "Epoch [430/500], Step [10/10], Loss: 0.001296909395023249\n",
      "Epoch [432/500], Step [10/10], Loss: 0.0013399325471254997\n",
      "Epoch [434/500], Step [10/10], Loss: 0.002251719226478599\n",
      "Epoch [436/500], Step [10/10], Loss: 0.0011808154958998784\n",
      "Epoch [438/500], Step [10/10], Loss: 0.0009961430914700031\n",
      "Epoch [440/500], Step [10/10], Loss: 0.001111504934669938\n",
      "Epoch [442/500], Step [10/10], Loss: 0.001187689667858649\n",
      "Epoch [444/500], Step [10/10], Loss: 0.0011296079464955255\n",
      "Epoch [446/500], Step [10/10], Loss: 0.0021931931623839773\n",
      "Epoch [448/500], Step [10/10], Loss: 0.0025785175821511075\n",
      "Epoch [450/500], Step [10/10], Loss: 0.0016649600438540801\n",
      "Epoch [452/500], Step [10/10], Loss: 0.001826112624257803\n",
      "Epoch [454/500], Step [10/10], Loss: 0.0009648859995650128\n",
      "Epoch [456/500], Step [10/10], Loss: 0.0011561149076442234\n",
      "Epoch [458/500], Step [10/10], Loss: 0.0013614806666737422\n",
      "Epoch [460/500], Step [10/10], Loss: 0.0010157861179322936\n",
      "Epoch [462/500], Step [10/10], Loss: 0.0022152965830173343\n",
      "Epoch [464/500], Step [10/10], Loss: 0.0018945405463455245\n",
      "Epoch [466/500], Step [10/10], Loss: 0.0013844121422152966\n",
      "Epoch [468/500], Step [10/10], Loss: 0.0023452438472304493\n",
      "Epoch [470/500], Step [10/10], Loss: 0.0015273755561793223\n",
      "Epoch [472/500], Step [10/10], Loss: 0.0013240626067272387\n",
      "Epoch [474/500], Step [10/10], Loss: 0.0012131584735470824\n",
      "Epoch [476/500], Step [10/10], Loss: 0.0012809859108529054\n",
      "Epoch [478/500], Step [10/10], Loss: 0.0013006749577471055\n",
      "Epoch [480/500], Step [10/10], Loss: 0.001146699934906792\n",
      "Epoch [482/500], Step [10/10], Loss: 0.0010736524855019525\n",
      "Epoch [484/500], Step [10/10], Loss: 0.0015251910226652399\n",
      "Epoch [486/500], Step [10/10], Loss: 0.001911885519803036\n",
      "Epoch [488/500], Step [10/10], Loss: 0.0017780289490474388\n",
      "Epoch [490/500], Step [10/10], Loss: 0.0013272755531943403\n",
      "Epoch [492/500], Step [10/10], Loss: 0.001296912036195863\n",
      "Epoch [494/500], Step [10/10], Loss: 0.0015276397170964628\n",
      "Epoch [496/500], Step [10/10], Loss: 0.0021008802359574474\n",
      "Epoch [498/500], Step [10/10], Loss: 0.0011038531010854058\n",
      "Epoch [500/500], Step [10/10], Loss: 0.0009241904263035394\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the model\n",
    "model = LSTM(input_size=m, hidden_size=50, num_layers=2, output_size=m, steps = steps).cuda()\n",
    "\n",
    "# Define the loss function and the optimizer\n",
    "criterion = nn.MSELoss()  # or another appropriate loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # adjust learning rate as needed\n",
    "\n",
    "num_epochs = 500\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    loss_epoch =0\n",
    "    for i, data_list in enumerate(train_loader):\n",
    "        \n",
    "        # Forward pass\n",
    "        out, out_back= model(data_list[0].to(device))\n",
    "        for k in range(steps):\n",
    "            # print(k,out[k].shape,data_list[k+1].shape)\n",
    "            if k == 0:\n",
    "                loss_fwd = criterion(out[k], data_list[k+1].to(device))\n",
    "            else:\n",
    "                loss_fwd += criterion(out[k], data_list[k+1].to(device))\n",
    "        \n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss_fwd.backward()\n",
    "        optimizer.step()\n",
    "        loss_epoch+=loss_fwd.item()\n",
    "\n",
    "    if (epoch+1) % 2 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss_epoch}')\n",
    "\n",
    "#     # Validation loop\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         total = 0\n",
    "#         correct = 0\n",
    "#         for images, labels in val_loader:\n",
    "#             images = images.to(device)\n",
    "#             labels = labels.to(device)\n",
    "#             outputs = model(images)\n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted == labels).sum().item()\n",
    "        \n",
    "#         print(f'Validation Accuracy of the model on the test images: {100 * correct / total}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2da44c1-05b0-42f0-b14a-8aecdc07c658",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Training Loop\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m): \u001b[38;5;66;03m# you can adjust this\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m inputs, targets \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m      8\u001b[0m         inputs, targets \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mcuda(), targets\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define Loss, Optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(100): # you can adjust this\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, 100, loss.item()))\n",
    "\n",
    "# Testing Loop\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        print('Test Loss: {:.4f}'.format(loss.item()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main_100",
   "language": "python",
   "name": "main_100"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
