{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6809f10-4267-4d45-81a3-86168cde11f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfc92717-8791-4f57-87ea-1d7d4d5926a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e58e52e-b99d-4d68-b405-ef49f97e80ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, bias):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        self.padding = kernel_size[0] // 2, kernel_size[1] // 2\n",
    "        self.bias = bias\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n",
    "                              out_channels=4 * self.hidden_dim,\n",
    "                              kernel_size=self.kernel_size,\n",
    "                              padding=self.padding,\n",
    "                              bias=self.bias)\n",
    "\n",
    "    def forward(self, input_tensor, cur_state):\n",
    "        h_cur, c_cur = cur_state\n",
    "\n",
    "        combined = torch.cat([input_tensor, h_cur], dim=1)\n",
    "\n",
    "        combined_conv = self.conv(combined)\n",
    "\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)\n",
    "        i = torch.sigmoid(cc_i)\n",
    "        f = torch.sigmoid(cc_f)\n",
    "        o = torch.sigmoid(cc_o)\n",
    "        g = torch.tanh(cc_g)\n",
    "\n",
    "        c_next = f * c_cur + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "\n",
    "        return h_next, c_next\n",
    "\n",
    "    def init_hidden(self, batch_size, image_size):\n",
    "        height, width = image_size\n",
    "        return (torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device),\n",
    "                torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device))\n",
    "\n",
    "class ConvLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers, batch_first=True, bias=True):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "\n",
    "        self._check_kernel_size_consistency(kernel_size)\n",
    "\n",
    "        kernel_size = self._extend_for_multilayer(kernel_size, num_layers)\n",
    "        hidden_dim = self._extend_for_multilayer(hidden_dim, num_layers)\n",
    "        if not len(kernel_size) == len(hidden_dim) == num_layers:\n",
    "            raise ValueError('Inconsistent list length.')\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_first = batch_first\n",
    "        self.bias = bias\n",
    "\n",
    "        cell_list = []\n",
    "        for i in range(0, self.num_layers):\n",
    "            cur_input_dim = self.input_dim if i == 0 else self.hidden_dim[i - 1]\n",
    "\n",
    "            cell_list.append(ConvLSTMCell(input_dim=cur_input_dim,\n",
    "                                          hidden_dim=self.hidden_dim[i],\n",
    "                                          kernel_size=self.kernel_size[i],\n",
    "                                          bias=self.bias))\n",
    "        self.fc = nn.Linear(self.hidden_dim[-1], 1)\n",
    "\n",
    "        self.cell_list = nn.ModuleList(cell_list)\n",
    "        \n",
    "        self.steps = steps\n",
    "\n",
    "    def forward(self, input_tensor, hidden_state=None):\n",
    "        if not self.batch_first:\n",
    "            # (t, b, c, h, w) -> (b, t, c, h, w)\n",
    "            input_tensor = input_tensor.permute(1, 0, 2, 3, 4)\n",
    "\n",
    "        input_tensor= input_tensor.unsqueeze(1)\n",
    "        b, seq_len, _, h, w = input_tensor.size()\n",
    "\n",
    "        if hidden_state is not None:\n",
    "            raise NotImplementedError()\n",
    "        else:\n",
    "            hidden_state = self._init_hidden(batch_size=b,\n",
    "                                             image_size=(h, w))\n",
    "\n",
    "        layer_output_list = []\n",
    "        last_state_list = []\n",
    "\n",
    "        seq_len = input_tensor.size(1)\n",
    "\n",
    "        q = input_tensor\n",
    "\n",
    "        \n",
    "        output_list= []\n",
    "        \n",
    "        for _ in range(self.steps):\n",
    "            \n",
    "            cur_layer_input = q\n",
    "\n",
    "            for layer_idx in range(self.num_layers):\n",
    "                h, c = hidden_state[layer_idx]\n",
    "                output_inner = []\n",
    "                for t in range(seq_len):\n",
    "                    h, c = self.cell_list[layer_idx](input_tensor=cur_layer_input[:, t, :, :, :],\n",
    "                                                     cur_state=[h, c])\n",
    "\n",
    "                    output_inner.append(h)\n",
    "\n",
    "                layer_output = torch.stack(output_inner, dim=1)\n",
    "                cur_layer_input = layer_output\n",
    "\n",
    "                layer_output_list.append(layer_output)\n",
    "                last_state_list.append([h, c])\n",
    "\n",
    "            output = layer_output_list[-1].permute(0,1,3,4,2)\n",
    "            output = self.fc(output)\n",
    "            output = output.permute(0,1,4,2,3)\n",
    "            q= output\n",
    "            output_list.append(q.squeeze(2))\n",
    "        \n",
    "        # if not self.batch_first:\n",
    "        #     # (b, t, c, h, w) -> (t, b, c, h, w)\n",
    "        #     layer_output_list = [x.permute(1, 0, 2, 3, 4) for x in layer_output_list]\n",
    "\n",
    "        return output_list\n",
    "\n",
    "    def _init_hidden(self, batch_size, image_size):\n",
    "        init_states = []\n",
    "        for i in range(self.num_layers):\n",
    "            init_states.append(self.cell_list[i].init_hidden(batch_size, image_size))\n",
    "        return init_states\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_kernel_size_consistency(kernel_size):\n",
    "        if not (isinstance(kernel_size, tuple) or\n",
    "                (isinstance(kernel_size, list) and all([isinstance(elem, tuple) for elem in kernel_size]))):\n",
    "            raise ValueError('`kernel_size` must be tuple or list of tuples')\n",
    "\n",
    "    @staticmethod\n",
    "    def _extend_for_multilayer(param, num_layers):\n",
    "        if not isinstance(param, list):\n",
    "            param = [param] * num_layers\n",
    "        return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "673810bb-fa16-45c8-84ce-22a9f62da249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1095, 70, 150) (1242, 70, 150)\n"
     ]
    }
   ],
   "source": [
    "X_train=np.load(\"/home/kumarv/renga016/Public/sst_dataset_omri/sstday_train.npy\")\n",
    "X_test=np.load(\"/home/kumarv/renga016/Public/sst_dataset_omri/sstday_valid.npy\")\n",
    "t, m, n = X_test.shape\n",
    "sst_nanflag = np.full((m, n), True)\n",
    "np.save(\"sst/sst_sst_omri_nanflag.npy\",sst_nanflag)\n",
    "print (X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9c75932-40e4-499c-be41-68ad89d5ae00",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape( X_train.shape[0],1, X_train.shape[1], X_train.shape[2])\n",
    "X_train = torch.from_numpy(X_train).float().contiguous()\n",
    "\n",
    "trainDat = []\n",
    "start = 0\n",
    "\n",
    "\n",
    "for i in np.arange(steps,-1, -1):\n",
    "    if i == 0:\n",
    "        trainDat.append(X_train[start:])\n",
    "    else:\n",
    "        trainDat.append(X_train[start:-i])\n",
    "    start += 1\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(*trainDat)\n",
    "# train_data = torch.tensor(trainDat)\n",
    "del(trainDat)\n",
    "\n",
    "train_loader = DataLoader(dataset = train_data,batch_size = 64,shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b038c81-fbb8-4530-a4c8-d38405513a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/100], Step [18/18], Loss: 0.4017778569832444\n",
      "Epoch [4/100], Step [18/18], Loss: 0.13382842065766454\n",
      "Epoch [6/100], Step [18/18], Loss: 0.09983065398409963\n",
      "Epoch [8/100], Step [18/18], Loss: 0.09700413839891553\n",
      "Epoch [10/100], Step [18/18], Loss: 0.08834020118229091\n",
      "Epoch [12/100], Step [18/18], Loss: 0.08621378010138869\n",
      "Epoch [14/100], Step [18/18], Loss: 0.08346066903322935\n",
      "Epoch [16/100], Step [18/18], Loss: 0.08307523094117641\n",
      "Epoch [18/100], Step [18/18], Loss: 0.08076982758939266\n",
      "Epoch [20/100], Step [18/18], Loss: 0.07835601130500436\n",
      "Epoch [22/100], Step [18/18], Loss: 0.07711825403384864\n",
      "Epoch [24/100], Step [18/18], Loss: 0.07554122805595398\n",
      "Epoch [26/100], Step [18/18], Loss: 0.07295144256204367\n",
      "Epoch [28/100], Step [18/18], Loss: 0.0708808375056833\n",
      "Epoch [30/100], Step [18/18], Loss: 0.07068808330222964\n",
      "Epoch [32/100], Step [18/18], Loss: 0.067276670364663\n",
      "Epoch [34/100], Step [18/18], Loss: 0.06514244014397264\n",
      "Epoch [36/100], Step [18/18], Loss: 0.06401304621249437\n",
      "Epoch [38/100], Step [18/18], Loss: 0.059474754612892866\n",
      "Epoch [40/100], Step [18/18], Loss: 0.05661047785542905\n",
      "Epoch [42/100], Step [18/18], Loss: 0.053298538317903876\n",
      "Epoch [44/100], Step [18/18], Loss: 0.05060152290388942\n",
      "Epoch [46/100], Step [18/18], Loss: 0.0467177398968488\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(steps):\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 26\u001b[0m         loss_fwd \u001b[38;5;241m=\u001b[39m criterion(out[k][:,:,sst_nanflag], \u001b[43mdata_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43msst_nanflag\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     28\u001b[0m         loss_fwd \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m criterion(out[k][:,:,sst_nanflag], data_list[k\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m][:,:,sst_nanflag]\u001b[38;5;241m.\u001b[39mto(device))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize the ConvLSTM model\n",
    "model = ConvLSTM(input_dim=1,   # input dimension (e.g., 3 for RGB images)\n",
    "                 hidden_dim=[64, 64],   # dimensionality of the hidden state in each layer\n",
    "                 kernel_size=(3, 3),   # size of the convolving kernel\n",
    "                 num_layers=2).to(device)   \n",
    "# Define the loss function and the optimizer\n",
    "criterion = nn.MSELoss()  # or another appropriate loss function\n",
    "optimizer = Adam(model.parameters(), lr=0.001)  # adjust learning rate as needed\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    loss_epoch =0\n",
    "    for i, data_list in enumerate(train_loader):\n",
    "        \n",
    "        # Forward pass\n",
    "        out= model(data_list[0].to(device))\n",
    "        for k in range(steps):\n",
    "            if k == 0:\n",
    "                loss_fwd = criterion(out[k][:,:,sst_nanflag], data_list[k+1][:,:,sst_nanflag].to(device))\n",
    "            else:\n",
    "                loss_fwd += criterion(out[k][:,:,sst_nanflag], data_list[k+1][:,:,sst_nanflag].to(device))\n",
    "        \n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss_fwd.backward()\n",
    "        optimizer.step()\n",
    "        loss_epoch+=loss_fwd.item()\n",
    "\n",
    "    if (epoch+1) % 2 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss_epoch}')\n",
    "\n",
    "#     # Validation loop\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         total = 0\n",
    "#         correct = 0\n",
    "#         for images, labels in val_loader:\n",
    "#             images = images.to(device)\n",
    "#             labels = labels.to(device)\n",
    "#             outputs = model(images)\n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted == labels).sum().item()\n",
    "        \n",
    "#         print(f'Validation Accuracy of the model on the test images: {100 * correct / total}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb2e65f-9c46-4153-8027-a9bada719bf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main_a100",
   "language": "python",
   "name": "main_a100"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
