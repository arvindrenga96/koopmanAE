{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd5efd62-fb9e-464b-82cf-974a5629f3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/100............. Loss: 1.0396\n",
      "Epoch: 20/100............. Loss: 1.0376\n",
      "Epoch: 30/100............. Loss: 1.0349\n",
      "Epoch: 40/100............. Loss: 1.0215\n",
      "Epoch: 50/100............. Loss: 1.0001\n",
      "Epoch: 60/100............. Loss: 0.9510\n",
      "Epoch: 70/100............. Loss: 0.8840\n",
      "Epoch: 80/100............. Loss: 0.7668\n",
      "Epoch: 90/100............. Loss: 0.6515\n",
      "Epoch: 100/100............. Loss: 0.5292\n",
      "Test Loss: 1.5937\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Define the BiLSTM model\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Define the LSTM layer\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n",
    "\n",
    "        # Define the output layer\n",
    "        self.linear = nn.Linear(hidden_dim*2, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_dim).to(x.device)\n",
    "\n",
    "        # Initialize cell state\n",
    "        c0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_dim).to(x.device)\n",
    "\n",
    "        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n",
    "        out, _ = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "\n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.linear(out[:, -1, :])\n",
    "\n",
    "        return out\n",
    "    \n",
    "\n",
    "# Generate synthetic dataset\n",
    "N = 1000 # Number of samples\n",
    "T = 10   # Sequence length\n",
    "input_dim = 1\n",
    "hidden_dim = 32\n",
    "num_layers = 2\n",
    "output_dim = 1\n",
    "\n",
    "# Randomly initialize the dataset\n",
    "X = torch.randn(N, T, input_dim)\n",
    "Y = torch.randn(N, output_dim)\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "train_x = X[:800]\n",
    "train_y = Y[:800]\n",
    "test_x = X[800:]\n",
    "test_y = Y[800:]\n",
    "\n",
    "# Initialize the model, loss function and optimizer\n",
    "model = BiLSTM(input_dim, hidden_dim, output_dim, num_layers)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = model(train_x)\n",
    "    loss = criterion(outputs, train_y)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print('Epoch: {}/{}.............'.format(epoch+1, epochs), end=' ')\n",
    "        print(\"Loss: {:.4f}\".format(loss.item()))\n",
    "\n",
    "# Testing\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predicted = model(test_x)\n",
    "    loss = criterion(predicted, test_y)\n",
    "print('Test Loss: {:.4f}'.format(loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a4cdadc-0b9b-488e-984a-fcfbb3177221",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expand(torch.FloatTensor{[998, 10, 1]}, size=[998, 10]): the number of sizes provided (2) must be greater or equal to the number of dimensions in the tensor (3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-5cc3b0070411>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m# Create input-output pairs for training that include both the next and previous states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0mY_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mY_pairs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY_forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0mY_pairs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY_backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expand(torch.FloatTensor{[998, 10, 1]}, size=[998, 10]): the number of sizes provided (2) must be greater or equal to the number of dimensions in the tensor (3)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Define the LSTM model\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Define the LSTM layer\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "\n",
    "        # Define the output layer\n",
    "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "\n",
    "        # Initialize cell state\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "\n",
    "        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n",
    "        out, _ = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "\n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.linear(out[:, -1, :])\n",
    "\n",
    "        return out\n",
    "\n",
    "# Generate synthetic dataset\n",
    "N = 1000 # Number of samples\n",
    "T = 10   # Sequence length\n",
    "input_dim = 1\n",
    "hidden_dim = 32\n",
    "num_layers = 2\n",
    "output_dim = 2  # We are predicting both the next and previous states\n",
    "\n",
    "# Randomly initialize the dataset\n",
    "X_prior = torch.randn(N, T, input_dim)\n",
    "X = X_prior[1:-1]\n",
    "Y_forward = X_prior[0:-2]  # Forward \n",
    "Y_backward = X_prior[2:]  #Backward\n",
    "\n",
    "\n",
    "# Create input-output pairs for training that include both the next and previous states\n",
    "Y_pairs = torch.zeros(X.shape[0], T, output_dim)\n",
    "Y_pairs[:, :, 0] = Y_forward\n",
    "Y_pairs[:, :, 1] = Y_backward\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "train_x = X[:800]\n",
    "train_y = Y_pairs[:800]\n",
    "test_x = X[800:]\n",
    "test_y = Y_pairs[800:]\n",
    "\n",
    "# Initialize the model, loss function and optimizer\n",
    "model = LSTM(input_dim, hidden_dim, output_dim, num_layers)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = model(train_x)\n",
    "    print (train_x.shape, outputs.shape,train_y.shape)\n",
    "    loss = criterion(outputs, train_y)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print('Epoch: {}/{}.............'.format(epoch+1, epochs), end=' ')\n",
    "        print(\"Loss: {:.4f}\".format(loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "823a903d-74e2-4db2-bc03-997d3af1a83c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([998, 10, 2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pairs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9143d9e8-1cfd-4984-889f-fc2631a3ff9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
